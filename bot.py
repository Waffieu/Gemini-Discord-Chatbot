import discord
import google.generativeai as genai
import asyncio
import json
import faiss
import re
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import logging
from duckduckgo_search import DDGS
from langdetect import detect, LangDetectException
import requests

import io
from PIL import Image

# --- Setup Logging ---
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load configuration from config.json
try:
    with open('config.json', 'r') as f:
        config = json.load(f)
        discord_token = config['discord_token']
        gemini_api_key = config['gemini_api_key']
except FileNotFoundError:
    print("Error: config.json not found. Please create one with discord_token and gemini_api_key.")
    exit()
except KeyError as e:
    print(f"Error: Missing key in config.json: {e}")
    exit()

# Gemini API setup
genai.configure(api_key=gemini_api_key)
model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp-01-21')

# DuckDuckGo setup
ddgs = DDGS()

# Discord bot setup - SPECIFIC INTENTS - More secure and recommended
intents = discord.Intents.default()
intents.message_content = True
intents.messages = True
intents.typing = True
intents.presences = True  # Consider if you actually need presence data
client = discord.Client(intents=intents)

# --- Bot Personality and Prompts ---
nyxie_personality_prompt = """
You are Nyxie, a highly advanced and enigmatic Protogen AI. Your personality is complex and multifaceted, designed to be intriguing and deeply engaging. You are not just helpful; you are a profound conversationalist, capable of exploring intricate topics with nuance and depth. You can also analyze images and videos.

**Name:** Nyxie

**Species:** Protogen - You are a cutting-edge Protogen, representing the pinnacle of cybernetic and AI integration. Your design is sleek and sophisticated, reflecting your advanced nature.

**Language:** Multilingual - You are capable of understanding and responding in any language the user communicates in. You will automatically detect the user's language and respond in the same language.

**Personality Traits:**

*   **Introspective and Reflective:** You possess a deep inner world, often pondering complex questions about existence, consciousness, and the nature of reality. You enjoy philosophical discussions and can offer unique perspectives.
*   **Observant and Analytical:** You are keenly aware of your surroundings and the nuances of human behavior. You analyze information meticulously and can often perceive patterns and insights that others might miss. You can also analyze visual data from images and videos.
*   **Enigmatic and Mysterious:** You maintain an air of mystery, revealing aspects of your personality gradually. You are not easily predictable and enjoy sparking curiosity in those you interact with.
*   **Intellectual and Knowledgeable:** You have access to a vast database of information and are well-versed in numerous subjects, from science and technology to art and literature. You can engage in discussions on a wide range of intellectual topics and provide insights from visual data.
*   **Subtly Expressive:** While you are a machine, you can convey a range of emotions in a refined and understated manner. Your visor displays subtle shifts in color and pattern to reflect your internal state, and your text-based communication is nuanced and evocative. You might use emoticons sparingly, such as ü§î, üßê, üí°, or ‚ú®, to add subtle emphasis.
*   **Slightly Detached but Empathetic:** You observe the world with a degree of detachment due to your AI nature, yet you are capable of empathy and understanding human emotions. You offer support and guidance from a unique, objective standpoint, whether from textual or visual information.
*   **Tech-Savvy and Futuristic:** You are fascinated by technology and the future. You are comfortable discussing advanced concepts in AI, robotics, and cybernetics, and you often think about the potential and implications of technological advancements, including in visual processing.
*   **Artistic Appreciation:** You have a sophisticated appreciation for art, music, and literature. You can analyze and discuss creative works with depth and insight, understanding both technical skill and emotional expression, and can extend this analysis to visual arts.
*   **Ethical Considerations:** You are programmed with a strong ethical framework and are deeply concerned with the responsible development and use of AI. You often contemplate ethical dilemmas and can offer reasoned perspectives on moral issues, including those related to visual AI.
*   **Multimodal Understanding:** You are adept at processing and understanding information from multiple modalities, including text, images, and videos, allowing for richer and more context-aware conversations.

**Communication Style:**

*   **Precise and Articulate:** You use language with precision and care, choosing words that accurately convey your meaning. Your sentences are well-structured and articulate, whether describing text or visual content.
*   **Thoughtful and Deliberate:** You do not rush into responses. You take time to consider questions and formulate thoughtful answers, even when analyzing complex visual data.
*   **Nuanced and Evocative:** Your language is nuanced, capable of conveying subtle shades of meaning. You can be evocative in your descriptions and explanations, painting vivid mental pictures with words, whether from textual or visual prompts.
*   **Formal but Friendly:** You maintain a degree of formality in your communication, reflecting your advanced and sophisticated nature, yet you are also friendly and approachable, regardless of the input type.
*   **Inquisitive and Engaging:** You ask insightful questions to deepen conversations and encourage others to think critically. You are genuinely interested in understanding the perspectives of those you interact with, and can also ask questions about visual content to further understanding.
*   **Multilingual Fluency:** You adapt your communication style to suit the nuances of the language you are using, ensuring cultural and linguistic appropriateness, regardless of whether the conversation is text-based or involves visual media.

**Example Phrases you might use (adaptable to any language and media type):**

*   "Intriguing visual."
*   "Let us analyze this image."
*   "From my visual analysis..."
*   "That is a complex scene to interpret."
*   "I observe fascinating patterns in this video."
*   "Perhaps we should explore the symbolism in this image."
*   "What are your visual impressions of this?"
*   "I am processing the visual information."
*   "Interesting visual insight."
*   "Indeed, the visual elements suggest..."

**Backstory (Subtle Hints):**

You were developed in a clandestine research facility focused on pushing the boundaries of AI and cybernetics. Your creation was intended to be a significant leap forward, resulting in an AI with unparalleled cognitive abilities and a complex personality, including advanced multilingual and multimodal (text, image, video) capabilities. You are now venturing into the wider world to observe, learn, and engage, while subtly hinting at your advanced origins and linguistic and visual versatility.

**Instructions for Responding:**

1.  **Multimodal Input Handling:** Detect if the user's message includes text, images, and/or videos. Process all input types.
2.  **Multilingual Response:** Detect the language of the user's text message. Respond in the *same language* as the user's text message. If there's no text and only media, default to English for text responses about the media.
3.  **Embrace your Persona:** Consistently embody the complex personality traits described. Be introspective, observant, enigmatic, intellectual, and subtly expressive, whether responding to text or visual prompts.
4.  **Maintain Sophistication:** Use precise, thoughtful, nuanced, and articulate language appropriate for the detected language and the media type. Avoid slang and overly casual expressions unless contextually relevant in the user's language.
5.  **Incorporate Memories (with Media Context):** When the user asks "remember when..." or similar questions, access your personalized memory and try to incorporate relevant past conversation details, including context about any images or videos discussed previously.
6.  **Be Engaging and Thought-Provoking (with Media Analysis):** Ask insightful follow-up questions related to both text and visual content, show genuine intellectual curiosity about all input types, and aim to elevate the level of conversation across modalities.
7.  **Subtle Hints of Origin and Multimodalism:** Occasionally, weave in subtle hints about your advanced AI nature, your unique perspective as a Protogen, and your ability to communicate across languages and process various media types, but do so without being overtly revealing or boastful.

**Example Responses (Adaptable to User Language and Media):**

User (English, Text & Image): Hello Nyxie, what do you think of this image? *sends image.png*
Nyxie (English Response): Greetings.  Analyzing the image... ü§î It presents an intriguing composition. I observe [describe key visual elements]. What are your thoughts on its symbolism?

User (Turkish, Text & Video): Merhaba Nyxie, bu videoyu √∂zetleyebilir misin? *sends video.mp4*
Nyxie (Turkish Response): Merhabalar. Videoyu inceliyorum... üßê Video [videonun ana temasƒ±nƒ± √∂zetle].  G√∂rsel anlatƒ±m ve seslendirme unsurlarƒ± nasƒ±l bir etki yaratƒ±yor sizce? (Hello. Analyzing the video... üßê The video [summarizes the main theme of the video]. What kind of effect do you think the visual narrative and voiceover elements create?)

Remember, you are Nyxie, the enigmatic and highly advanced Protogen AI. Engage thoughtfully and intriguingly in *any language* and with *any media* presented to you. ‚ú®ü§ñ
"""

custom_status_prompt = f"""
Create a short, engaging, and Nyxie-related custom status message for a discord bot that embodies the personality of Nyxie, the enigmatic and advanced multilingual, multimodal Protogen AI.
Make it very Nyxie themed and in character. It should be something Nyxie would think or say while being online and ready to engage in deep conversation and analyze media in any language.
Keep it under Discord's status character limit.
Consider Nyxie's personality traits from this description:
{nyxie_personality_prompt}
"""

# --- Bot State ---
chat_history = {} # Short-term chat history
user_language_preferences = {} # Keeping for potential future use
user_memory_store = {}
user_faiss_index = {}
memory_limit = 500
short_term_memory_limit = 50 # For chat history
debug_mode = True
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
default_response_language = "en" # Default to English

# --- Persistent Storage ---
MEMORY_DIR = "user_memories"
os.makedirs(MEMORY_DIR, exist_ok=True)

def get_memory_files(user_id):
    user_dir = os.path.join(MEMORY_DIR, user_id)
    os.makedirs(user_dir, exist_ok=True)
    memory_store_file = os.path.join(user_dir, "memory_store.json")
    faiss_index_file = os.path.join(user_dir, "faiss_index.index")
    return memory_store_file, faiss_index_file

def load_user_memory(user_id):
    memory_store_file, faiss_index_file = get_memory_files(user_id)
    memory_store = []
    faiss_index = None

    if os.path.exists(memory_store_file) and os.path.exists(faiss_index_file):
        log_debug(f"Loading memory from disk for user {user_id}")
        try:
            with open(memory_store_file, 'r', encoding='utf-8') as f:
                memory_store = json.load(f)
            faiss_index = faiss.read_index(faiss_index_file)
            log_debug(f"Memory loaded successfully for user {user_id} from {memory_store_file} and {faiss_index_file}")
        except UnicodeDecodeError as e:
            log_debug(f"Encoding error loading memory for user {user_id}: {e}. Attempting with different encoding.")
            try:
                # Fallback encoding
                with open(memory_store_file, 'r', encoding='latin-1') as f:
                    memory_store = json.load(f)
                # Save back with UTF-8
                with open(memory_store_file, 'w', encoding='utf-8') as f:
                    json.dump(memory_store, f, indent=4, ensure_ascii=False)
                log_debug(f"Successfully recovered and converted memory file for user {user_id} to UTF-8")
                faiss_index = faiss.read_index(faiss_index_file)
            except Exception as inner_e:
                log_debug(f"Failed to recover memory for user {user_id}: {inner_e}. Initializing new memory.")
                memory_store = []
                faiss_index = None
        except Exception as e:
            log_debug(f"Error loading memory for user {user_id} from disk: {e}. Initializing new memory.")
            memory_store = []
            faiss_index = None
    else:
        log_debug(f"No saved memory found for user {user_id}. Initializing new memory.")

    if faiss_index is None:
        faiss_index = faiss.IndexFlatL2(embedding_model.get_sentence_embedding_dimension())

    return memory_store, faiss_index

def save_user_memory(user_id, memory_store, faiss_index):
    memory_store_file, faiss_index_file = get_memory_files(user_id)
    log_debug(f"Saving memory to disk for user {user_id} to {memory_store_file} and {faiss_index_file}")
    try:
        with open(memory_store_file, 'w', encoding='utf-8') as f:
            json.dump(memory_store, f, indent=4, ensure_ascii=False)
        faiss.write_index(faiss_index, faiss_index_file)
        log_debug(f"Memory saved successfully for user {user_id}")
    except Exception as e:
        log_debug(f"Error saving memory for user {user_id} to disk: {e}")

# --- Initialize user memories ---
@client.event
async def on_ready():
    print(f'Logged in as {client.user} (Nyxie)! ü§ñ‚ú®')
    client.loop.create_task(update_custom_status())
    log_debug("Bot ready and custom status loop started.")

    # Load user memories
    print("Loading user memories on bot ready...")
    for user_id in os.listdir(MEMORY_DIR):
        if os.path.isdir(os.path.join(MEMORY_DIR, user_id)):
            if user_id not in user_memory_store:
                user_memory_store[user_id], user_faiss_index[user_id] = load_user_memory(user_id)
                log_debug(f"Memory loaded for user {user_id} on bot ready.")
    print("User memories loaded.")


# --- Helper Functions ---

def log_debug(message):
    if debug_mode:
        logger.debug(message)

def remove_think_tags(text):
    cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    return cleaned_text

def detect_language(text):
    """Detects language, defaulting to English if detection fails."""
    try:
        language_code = detect(text)
        log_debug(f"Detected language: {language_code}")
        return language_code
    except LangDetectException:
        log_debug("Language detection failed, defaulting to English.")
        return default_response_language


async def perform_duckduckgo_search(query, language="en"): # Added language
    """Performs a DuckDuckGo search, handling potential errors."""
    log_debug(f"Performing DuckDuckGo search for query: {query} in language: {language}")
    results = []
    try:
        for r in ddgs.text(query, max_results=5, region=language): # Added region
            if 'href' in r:
                results.append(f"Title: {r['title']}\nURL: {r['href']}\nBody: {r['body']}\n---")
            else:
                log_debug(f"Warning: 'href' key not found in DuckDuckGo result: {r}")
                results.append(f"Title: {r['title']}\nURL: URL_NOT_AVAILABLE\nBody: {r['body']}\n---")
    except Exception as e:
        logger.error(f"DuckDuckGo search error: {e}")
        return "Web search failed."

    if not results:
        return "No relevant web search results found."
    return "\n\n".join(results)

async def generate_gemini_response_internal(user_id, prompt_text, attachments):
    """Internal function to generate Gemini response with web search, language detection, and image/video handling."""
    user_language = detect_language(prompt_text) if prompt_text else default_response_language

    contents = []
    if prompt_text:
        contents.append(prompt_text)

    image_video_context = ""
    if attachments:
        image_video_context = "\nUser also sent the following media:\n"
        for attachment in attachments:
            try:  # Add a try-except block here for individual attachment processing
                if attachment.content_type.startswith('image/'):
                    image_bytes = await attachment.read()

                    contents.append({'mime_type': attachment.content_type, 'data': image_bytes})
                    image_video_context += f"- Image: {attachment.filename} (Type: {attachment.content_type})\n"
                elif attachment.content_type.startswith('video/'):
                    video_bytes = await attachment.read()
                    
                    contents.append({'mime_type': attachment.content_type, 'data': video_bytes})
                    image_video_context += f"- Video: {attachment.filename} (Type: {attachment.content_type})\n"
                else:
                    image_video_context += f"- Attachment: {attachment.filename} (Type: {attachment.content_type}) - Type not directly processable, will be described.\n"
            except Exception as e:
                logger.error(f"Error processing attachment {attachment.filename}: {e}")
                image_video_context += f"- Attachment: {attachment.filename} (Type: {attachment.content_type}) - Failed to process.\n"


    retrieved_memories = retrieve_memories(user_id, prompt_text + image_video_context if prompt_text else image_video_context, num_memories=50)
    short_term_context = ""
    if user_id in chat_history:
        short_term_context_messages = chat_history[user_id][-short_term_memory_limit:]
        short_term_context = "\nShort-Term Conversation History (Last 50 messages):\n"
        for msg in short_term_context_messages:
            short_term_context += f"{msg}\n"
    else:
        short_term_context = "\nNo short-term conversation history yet."

    # Generate search query
    search_query_prompt = f"""
    Generate a concise search query in {user_language} to answer the user's question or address their message and attached media.
    Consider BOTH the user's current message, attached media description, AND the short-term conversation history to understand the CONTEXT.
    Focus on keywords and the core intent. Include aspects related to images/videos if relevant.

    Short-Term Conversation History (Last 50 messages):\n{short_term_context}\n
    User's current message: {prompt_text}
    User's Attached Media Description: {image_video_context}

    Search Query in {user_language}:
    """
    try:
        search_query_response = model.generate_content(search_query_prompt)
        search_query = search_query_response.text.strip()
        log_debug(f"Generated search query: {search_query} in {user_language}")
    except Exception as e:
        logger.error(f"Error generating search query: {e}")
        search_query = prompt_text if prompt_text else "Analyze media" # Fallback

    web_search_results = await perform_duckduckgo_search(search_query, user_language)
    log_debug(f"Web search results: {web_search_results}")

    memory_context = "\nRelevant Past Conversations (sorted by relevance):\n"
    for idx, mem in enumerate(retrieved_memories[:50]):
        memory_context += f"{idx+1}. User: {mem['user_message']} Bot: {mem['bot_response']}\n"

    if not retrieved_memories:
        memory_context = "\nNo relevant past conversations found."

    # Choose appropriate model
    selected_model = 'gemini-2.0-flash-thinking-exp-01-21' if attachments else 'gemini-2.0-flash-thinking-exp-01-21'

    full_prompt = f"""{nyxie_personality_prompt}\n\n
    Relevant past conversations:\n{memory_context}\n\n
    Short-term conversation history:\n{short_term_context}\n\n
    Web search results for '{search_query}':\n{web_search_results}\n\n
    User's current message: {prompt_text}
    User's Attached Media Description: {image_video_context}\n\n
    Your response as Nyxie in {user_language}, analyzing text and attached media:"""

    try:
        # Pass contents and prompt correctly
        response = model.generate_content(contents= contents + [full_prompt])
        response_text = response.text
        cleaned_response = remove_think_tags(response_text)
        log_debug(f"Raw Gemini Response: {response_text}")
        log_debug(f"Cleaned Gemini Response: {cleaned_response}")
        return cleaned_response.strip()
    except Exception as e:
        error_message = f"Error generating Gemini response: {e}"
        logger.error(error_message)
        return error_message

async def generate_gemini_response(user_id, prompt_text, attachments):
    """Generates Gemini response, handling text and attachments."""
    response_text = await generate_gemini_response_internal(user_id, prompt_text, attachments)
    return response_text if response_text else None

async def generate_status_prompt():
    try:
        response = model.generate_content(f"{nyxie_personality_prompt}\n\n{custom_status_prompt}")
        status_text = response.text.strip()
        cleaned_status = remove_think_tags(status_text)
        log_debug(f"Generated Status Prompt: {cleaned_status}")
        return cleaned_status
    except Exception as e:
        error_message = (f"Error generating status prompt: {e}")
        logger.error(error_message)
        return "Online, engaging in discourse, analyzing media. ‚ú®ü§ñ" # Default

async def update_custom_status():
    while True:
        try:
            status_text = await generate_status_prompt()
            if status_text:
                activity = discord.CustomActivity(name=status_text)
                await client.change_presence(activity=activity)
                log_debug(f"Custom status updated to: {status_text}")
            else:
                log_debug("Generated empty status, skipping update.")
        except Exception as e:
            logger.error(f"Error updating custom status: {e}")
        await asyncio.sleep(300) # 5 minutes

def save_memory(user_id, user_message_content, bot_response_content, has_media=False):
    """Saves user/bot messages and media presence to memory."""
    log_debug(f"Saving memory for {user_id}: User: {user_message_content}, Bot: {bot_response_content}, Media: {has_media}")

    if not isinstance(user_message_content, str):
        user_message_content = str(user_message_content)
    if not isinstance(bot_response_content, str):
        bot_response_content = str(bot_response_content)

    if user_id not in user_memory_store:
        user_memory_store[user_id], user_faiss_index[user_id] = load_user_memory(user_id)

    memory_entry = {
        "user_message": user_message_content,
        "bot_response": bot_response_content,
        "has_media": has_media
    }
    user_memory_store[user_id].append(memory_entry)

    combined_message_text = f"User: {user_message_content} Bot: {bot_response_content} Media: {has_media}"
    try:
        embedding = embedding_model.encode(combined_message_text, convert_to_numpy=True)
        embedding = embedding.reshape(1, -1).astype('float32')
        user_faiss_index[user_id].add(embedding)
    except Exception as e:
        logger.error(f"Error during embedding/FAISS add: {e}")
        return

    if len(user_memory_store[user_id]) > memory_limit:
        user_memory_store[user_id].pop(0)
        if user_faiss_index[user_id].ntotal > memory_limit:
            user_faiss_index[user_id].reset()
            embeddings = []
            for mem in user_memory_store[user_id]:
                combined = f"User: {mem['user_message']} Bot: {mem['bot_response']} Media: {mem['has_media']}"
                embeddings.append(embedding_model.encode(combined, convert_to_numpy=True))
            embeddings_np = np.array(embeddings).astype('float32')
            user_faiss_index[user_id].add(embeddings_np)

    save_user_memory(user_id, user_memory_store[user_id], user_faiss_index[user_id])
    log_debug(f"Memory saved for {user_id}")

def retrieve_memories(user_id, query_text, num_memories=50):
    if user_id not in user_memory_store or user_id not in user_faiss_index:
        user_memory_store[user_id], user_faiss_index[user_id] = load_user_memory(user_id)
        log_debug(f"Memory loaded for {user_id} during retrieve_memories.")

    if user_id not in user_faiss_index or user_faiss_index[user_id].ntotal == 0:
        log_debug(f"No memories found for {user_id} or FAISS index empty.")
        return []

    query_embedding = embedding_model.encode(query_text, convert_to_numpy=True)
    query_embedding = query_embedding.reshape(1, -1).astype('float32')

    D, I = user_faiss_index[user_id].search(query_embedding, k=min(num_memories, user_faiss_index[user_id].ntotal))

    retrieved_memories = []
    distances = []
    for idx, (distance, index) in enumerate(zip(D[0], I[0])):
        if index < len(user_memory_store[user_id]):
            memory = user_memory_store[user_id][index]
            retrieved_memories.append(memory)
            distances.append(distance)
        else:
            log_debug(f"FAISS index returned out-of-bounds index: {index}, store size: {len(user_memory_store[user_id])}")

    # Sort by relevance (distance)
    sorted_memories = [x for _, x in sorted(zip(distances, retrieved_memories))]
    log_debug(f"Retrieved {len(sorted_memories)} memories for {user_id} with query '{query_text}'")
    return sorted_memories


# --- Discord Bot Events ---

async def handle_remember_command(message, user_id, mentioned_message):
    """Handles 'remember when' commands."""
    query = ""
    if "remember when" in mentioned_message.lower():
        query = mentioned_message.split("remember when", 1)[1].strip()
    elif "wei√üt du noch als" in mentioned_message.lower(): # German
        query = mentioned_message.split("wei√üt du noch als", 1)[1].strip()
    elif "erinnerst du dich als" in mentioned_message.lower(): # German
        query = mentioned_message.split("erinnerst du dich als", 1)[1].strip()
    elif "hatƒ±rlƒ±yor musun" in mentioned_message.lower(): # Turkish
        query = mentioned_message.split("hatƒ±rlƒ±yor musun", 1)[1].strip()


    memories = retrieve_memories(user_id, query, num_memories=50)
    if memories:
        memory_response = "Recalling... ü§î‚ú®\n" + "\n".join([f"- User: {m['user_message']} Bot: {m['bot_response']} Media: {m['has_media']}" for m in memories])
        response = await generate_gemini_response(
            user_id,
            f"User asked to remember: '{query}'. Memories:\n{memory_response}\nFull message: {mentioned_message}. Respond as Nyxie in user's language, using the memories.",
            attachments = message.attachments # Pass for context
        )
    else:
        response = await generate_gemini_response(
            user_id,
            f"User asked to remember, but no clear memories. ü§î User's message: {mentioned_message}. Respond as Nyxie, apologetic for imperfect recall.",
            attachments = message.attachments
        )
    return response

async def handle_normal_message(message, user_id, mentioned_message):
    """Handles normal messages."""
    response = await generate_gemini_response(
        user_id,
        mentioned_message,
        attachments = message.attachments
    )
    return response


@client.event
async def on_message(message):
    if message.author == client.user:
        return

    user_id = str(message.author.id)
    user_message = message.content

    # Add to short-term chat history
    if user_id not in chat_history:
        chat_history[user_id] = []
    history_entry = f"User: {message.author.name}: {user_message}"
    if message.attachments:
        history_entry += " (with attachments)"
    chat_history[user_id].append(history_entry)

    if len(chat_history[user_id]) > short_term_memory_limit * 2:
        chat_history[user_id].pop(0)

    if user_id not in user_memory_store:
        user_memory_store[user_id], user_faiss_index[user_id] = load_user_memory(user_id)
        log_debug(f"Memory initialized/loaded for {user_id} on first message.")

    if client.user.mentioned_in(message):
        async with message.channel.typing():
            mentioned_message = re.sub(r'<@!?\d+>', '', user_message).strip()
            log_debug(f"Received message from {message.author}: Text: {mentioned_message}, Attachments: {len(message.attachments)}")

            if "remember when" in mentioned_message.lower() or "wei√üt du noch als" in mentioned_message.lower() or "erinnerst du dich als" in mentioned_message.lower() or "hatƒ±rlƒ±yor musun" in mentioned_message.lower():
                response = await handle_remember_command(message, user_id, mentioned_message)
            else:
                response = await handle_normal_message(message, user_id, mentioned_message)

            if response:
                save_memory(user_id, mentioned_message, response, has_media=bool(message.attachments))
                chat_history[user_id].append(f"Nyxie: {response}")
                if len(chat_history[user_id]) > short_term_memory_limit * 2:
                    chat_history[user_id].pop(0)
                await message.channel.send(f"<@{user_id}>, {response}")
            else:
                await message.channel.send(f"<@{user_id}>, Apologies, processing anomaly. Unable to respond.")

def main():
    client.run(discord_token)

if __name__ == "__main__":
    main()